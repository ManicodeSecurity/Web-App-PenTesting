# Application Spidering

The spider or "crawl" phase of a scan involves navigating around the application, following links, submitting forms, and logging in where necessary, to catalog the content of the application and the navigational paths within it. This seemingly simple task presents a variety of challenges that Burp's crawler is able to meet, to create an accurate map of the application.

Learn more about how Burp craws websites [here](https://portswigger.net/burp/documentation/scanner/crawling).

### Spider our Target Domain

1. First, we want to browse around the application as an authenticated user to populate as much of the HTTP history as we can. 

Log into the Security Shepherd application and start clicking some of the challenges as well as entering dummy inputs. Your HTTP history should start to look like this:

![Burp History](../images/burp_history.png?raw=true "Burp History")

2. Using the Spider feature in Burp Suite is straightforward. In the `Proxy -> HTTP history` tab, right click on the host `manicode.us` with the URL `/shepherd`. 

Now, click `Spider from here`. Burp will ask you how to handle a few forms it encounters. Fill in your credentials for the username and password and ignore the rest of the forms. 

3. Go to the `Target` tab in Burp and you will see a Site map starting to populate.

Note: Spidering can be intrusive on a production web application. Make sure to check out the `Spider` tab in Burp Suite to ensure you have the settings dialed in. Security Shepherd uses a number of AJAX requests which Burp does not always handle gracefully. 

### Bonus
Try to Spider a different site that you have permission to test.


